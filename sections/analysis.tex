\section{Analysis}\label{sec:analysis}
%The next line produces an indented paragraph to start the document
 %unit.  The LaTeX defaults start most units without indentations.
\hspace{\parindent}
Outline the analysis section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Comparison to Simulation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Comparison of Data to Simulation}
  To determine what underlying true physics values cause the data that we
  measure, we need to compare the data to Monte Carlo simulation. The physics
  model of the data is a combination of many different models including nuclear
  physics models, neutrino cross section models, nucleon structure models,
  cosmic ray models, which would be very complicated to calculate directly.
  Instead, we simulate the data we expect to see in the detector given a set of
  physical parameters and models and compare that directly to the actual data
  we measured. We do this for many possible values of the parameters and
  calculate the likelihood for each set of parameter values. In addition to
  varying the parameters that we want to measure in the simulation, we vary the
  physical parameters whose true values aren't well constrained which might
  have a large effect on the final data. This allows us to quantify the
  uncertainty due to the unknown quantities.
  \subsubsection{Event Reweighting}\label{sec:reweighting}
    Full Monte Carlo simulations are both time and compute intensive. Instead
    of re-running the simulation for each possible parameter value that we are
    interested in, we can calculate the ratio of the probabilities of each
    interaction occurring given the new parameter values to the probabilities
    of each interaction occurring for the original simulated parameter values.
    We refer to this ratio as an event weight:
    \begin{equation}
      w = \frac{P(\textrm{event}|\theta')}{P(\textrm{event}|\theta)} \,,
    \end{equation}
    where $w$ is the event weight for a given event, $P($event$|\theta)$ is the
    probability of the simulated event given the set of original parameters
    used in the simulation, $\theta$, and $P($event$|\theta')$ is the
    probability of the simulated event given a new set of parameters,
    $\theta'$.

    The four contributions to the GENIE cross section model (the nuclear
    physics model, the neutrino-nucleon cross section model, the hadron
    production model, and the intranuclear hadron transport model from
    Sec.~\ref{sec:geniexsec}) are treated independently in NC elastic
    events~\cite{Andreopoulos:2009rq}. This allows us to factor the event
    weight,
    \begin{equation}\label{eq:weights}
      w = w_{\textrm{nuclear}}\times w_{\textrm{neutrino-nucleon}}\times 
          w_{\textrm{hadron prod.}}\times w_{\textrm{intranuclear}} \,,
    \end{equation}
    where $w_{\textrm{nuclear}}$ is the nuclear physics model weight,
    $w_{\textrm{neutrino-nucleon}}$ is the neutrino-nucleon cross section model
    weight, $w_{\textrm{hadron prod.}}$ is the hadron production model weight,
    and $w_{\textrm{intranuclear}}$ is the intranuclear hadron transport model
    weight. Only the NC elastic cross section probability ratio needs to be
    calculated to see the effect of $\Delta s$ or another cross section
    parameter of interest on reconstructed $Q^2$ of selected events.

    The probability of a neutrino interaction is proportional to the
    interaction cross section, so the weight, $w_{\textrm{neutrino-nucleon}}$
    is simply a ratio of the cross sections
    \begin{equation}
      w_{\textrm{neutrino-nucleon}} = w_{\sigma} 
        = \frac{d^n\sigma'_{\nu}/dK^n}{d^n\sigma_{\nu}/dK^n} \,,
    \end{equation}
    where $d^n\sigma/dK^n$ is the differential cross section for the initially
    simulated neutrino-nucleon interaction, and $d^n\sigma'/dK^n$ is the
    differential cross section with the modified parameters evaluated at the
    kinematical phase space $\{K^n\}^3$. The differential cross section is a
    function of the neutrino energy in the rest frame of the scattered nucleon,
    $E_{\nu}^{(NRF)}$, the interaction four-momentum transfer, $Q^2$, and the
    physics model, including the model parameters.

    To determine the effect of the NC elastic axial form factor parameters
    $\Delta s$ and $M_A^s$ (or $a_0^s$, $a_1^s$, and $a_2^s$) on the data, we
    calculate the NC elastic cross section given these new parameters and the
    cross section given the initial simulation parameters for each NC elastic
    event in the Monte Carlo simulation. To get an accurate weight, the
    denominator needs to be calculated exactly as the cross section was
    calculated in the initial GENIE simulation. However, there is no reason
    that a different model can't be used for the numerator. For the numerator
    in elastic interactions, we use the LLewellyn-Smith neutrino-nucleon
    elastic cross section parameterization described in Sec.~\ref{sec:probe}
    with the z expansion vector and axial form factors described in
    Sec.~\ref{sec:zexpansion}.

    To determine the effect of a set of NC elastic cross section parameters
    given our parameterization, we calculate the neutrino-nucleon weight for
    each NC elastic event
    \begin{equation}\label{eq:xsecweight}
      w_\sigma = \frac{\left(\frac{d\sigma}{dQ^2}(a_0^s,a_1^s,a_2^s)\right)}
                      {\left(\frac{d\sigma}{dQ^2}\right)_{GN}} \,.
    \end{equation}

    We also calculate weights to determine the effect on the simulated data of
    each of the source of systematic uncertainty described in
    Sec.~\ref{sec:systematics}.  We can sample the probability space of each of
    these ``nuisance" parameters and calculate a weight based on that value.
    If the nuisance parameters are independent of each other, multiplying the
    nuisance parameter weights together is equivalent to sampling the combined,
    $N$-dimensional probability space of the systematic parameters, where $N$
    is the number of parameters. Sampling each of the relevant systematic
    parameters from Sec.~\ref{sec:systematics} gives
    \begin{equation}\label{eq:systweight}
      w_{syst} = w_{\textrm{flux}} \times w_{\textrm{D.I.C.}}\times w_{\textrm{S.P.E.}}\times
                 w_{\textrm{MEC}} \times w_{\textrm{P.B.}} \times w_{\textrm{dirt}}\,.
    \end{equation}
    Combining Eqns.~\ref{eq:weights},~\ref{eq:xsecweight},
    and~\ref{eq:systweight} gives an event weight which combines the effect
    due to a sample from the model parameter distribution and the values of the
    strange axial form factor parameters that we want to measure
    \begin{equation}
      w = w_{\sigma}\times w_{syst} \,.
    \end{equation}

  \subsubsection{Likelihood calculation}\label{sec:likelihood}
    To compare the simulation and the weight calculations directly to the data,
    we sum weights for each reconstructed $Q^2$ bin in the distribution of
    events passing the NC elastic proton selection
    \begin{equation}\label{eq:expected}
      N_{NCE}(Q^2_i) = \sum\limits_{j\in NC_i} w_j \,,
    \end{equation}
    where $i$ is the $Q^2$ bin, $NC_i$ is the set of events selected as NC
    elastic neutrino-proton events in the $i$th $Q^2$ bin, and $w = w_j$ is the
    calculated weight of the event. If the event is not a true simulated
    elastic event, then $w_\sigma = 1$ giving $w_j = w_{syst}$. At this point,
    the Monte Carlo simulated data is directly comparable to the detector data.

    To evaluate how well the model represents the data, we calculate the
    probability of the observed data given a model and set of parameters.  This
    probability is called the likelihood. We assume that the measured data is
    normally distributed, and the likelihood is
    \begin{equation}\label{eq:likelihood}
      P(D^{obs}|\theta) = \prod_{i\in I_{Q^2}} P(D^{obs}_i|\theta) = \frac{1}{\sqrt{2\pi \sigma_i^2}}
             e^{-\frac{1}{2}(D^{obs}_i - D^{exp}_i(\theta))^2/\sigma_i^2} \,,
    \end{equation}
    where $D^{obs}$ is the measured NC elastic neutrino-proton event selection
    distribution in data, $I_{Q^2}$ is the set of $Q^2$ bins, $D^{obs}_i$ is
    the measured number of selected events of that bin, and $\sigma_i$ is the
    uncertainty of that measured bin value. The expected value of the event
    selection distribution, $D^{exp}_i(\theta)$, is a function of the model and
    the set of model parameters, $\theta$. It is equal to $N_{NCE}(Q^2_i)$ in
    Eqn.~\ref{eq:expected}. The set of model parameters, $\theta$, contains the
    systematic model parameters described in Sec.~\ref{sec:systematics} and the
    strange axial form factor parameters, $a_0^s$, $a_1^s$, and $a_2^s$
    described in Sec.~\ref{sec:ncaxial}.
    

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Joint Estimation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Strange axial form factor parameter estimation}\label{sec:deltas}
  The likelihood give the probability of the observed data given a model and a
  set of parameters. What we are really interested in is the probability of the
  parameters $a_0^2$, $a_1^s$, and $a_2^s$ given the observed data and our
  model. We can determine this probability distribution using Bayesian
  inference and probability sampling methods.

  \subsubsection{Bayesian inference}
    Bayes' theorem is used to convert between the likelihood and the
    probability of the model given the data
    \begin{equation}
      P(\theta|D^{obs}) = \frac{P(D^{obs}|\theta)P(\theta)}{P(D^{obs})} \,.
    \end{equation}
    The likelihood, $P(D^{obs}|\theta)$, was described in detail in
    Sec.~\ref{sec:likelihood} and is defined in Eqn.~\ref{eq:likelihood}. The
    other three factors in Bayes' theorem deserve some explanation.
    
    First, the probability of $\theta$ given the observed data,
    $P(\theta|D^{obs})$, is the probability distribution that we ultimately
    want to determine. It is referred to as the posterior distribution.
    Implicit in the notation is the model. If we wanted to write it explicitly,
    it would be:
    \begin{equation*}
      P(\theta|D^{obs})\equiv P(\theta|D^{obs},\mathcal{M}) \,,
    \end{equation*}
    where $\mathcal{M}$ represents the physics model. The parameter set
    $\theta$ is still the set containing the strange axial form factor
    parameters, and the systematic parameters from Sec.~\ref{sec:reweighting}.

    Next, the probability of the parameters, $P(\theta)$, is referred to as the
    prior distribution. It is also implicitly conditional on the model,
    $\mathcal{M}$. This is where we include prior information that we know to
    be true. It is impossible not to include some prior information in
    inference. For example, using a uniform prior on $\Delta s$ is the same as
    saying that $\Delta s$ has the same probability of being zero as it does of
    being infinite. The prior should be used to exclude unphysical parameter
    values, like negative mass. In a good model with adequate data the
    posterior distribution should be robust to the choice of a prior. It is
    always necessary to evaluate the effect of the choice of priors is on the
    posterior. In Sec.~\ref{sec:results} we show the effect of different
    priors, including a uniform prior, on the posterior distribution.

    Last, the marginal probability of the observed data $P(D^{obs})$ integrated
    over all $\theta$ values.  It too is implicitly conditional on
    $\mathcal{M}$ and is referred to as the evidence of the model. Explicitly,
    it can be written as
    \begin{equation*}
      P(R^{obs}) \equiv P(D^{obs}|\mathcal{M}) 
          = \int_{\theta}P(D^{obs}|\theta)P(\theta|\mathcal{M})d\theta \,.
    \end{equation*}

  \subsubsection{Markov Chain Monte Carlo}
    There is no way to calculate an exact solution to our posterior
    distribution analytically, but it can be estimated it numerically by
    sampling. Most sampling techniques would be computationally impossible due
    to the fact that our posterior distribution is nine-dimensional ($a_0^a$,
    $a_1^s$, $a_2^s$, and the six systematic nuisance parameters), and each
    likelihood calculation requires thousands of event weights to be
    calculated.  Markov chain Monte Carlo (MCMC) is a class of methods for
    sampling multi-dimensional posterior distributions. Two of the most common
    MCMC algorithms are the Metropolis algorithm~\cite{Metropolis:1953am} and
    Gibbs sampling~\cite{Geman:1987}. Both are used in this analysis.

    The Metropolis algorithm is a random walk in the $N$-dimensional parameter
    space with a rule to either accept or reject each step in the walk. Each
    proposed step is drawn from a proposal distribution.  In this analysis, we
    use a multivariate normal proposal distribution centered at the current
    position.  The step is accepted if the value of the posterior distribution
    at the proposed position is greater than at the current position. If the
    value at the proposed distribution is less than the current value, the
    proposed step is accepted with a probability equal to the ratio of the
    value at the proposed position to the value at the current position. The
    decision to accept or reject a proposed step can be determined entirely by
    calculating the \textit{ratio} of the posterior values at the proposed and
    current positions. Since the evidence, $P(D^{obs})$, doesn't depend on
    $\theta$, it never needs to be calculated. At every proposed step only the
    likelihood, $P(D^{obs}|\theta)$, and the prior, $P(\theta)$, need to be
    calculated.

    The Metropolis sampling methods is used for both the parameter estimation
    of the strange axial form factor parameters and the systematic nuisance
    parameters. We don't expect the posterior distributions of the systematic
    parameters to look different than their priors, and sampling them allows us
    to account for the effect that they have on the data.  First, a Metropolis
    step is proposed for new $a_0^2$, $a_1^s$, and $a_2^s$ values. The proposed
    step is determined by drawing from a three-dimensional Gaussian
    distribution centered at the current position of $a_0^s$, $a_1^s$, and
    $a_s^2$. The width of the Gaussian in each dimension is optimized to the
    data and the parameters. The likelihood value at the proposed position is
    evaluated at the current position of the systematic nuisance parameters.
    Then, a step is taken in all of the systematic nuisance parameters at once,
    and the likelihood is evaluated at the current position of $a_0s$, $a_1^s$,
    and $a_2^s$. The proposed positions of each of the six parameters in the
    step are independent of each other and of the current parameter values. The
    proposed systematic steps are drawn from Gaussian distributions
    representing the estimated true distribution of the parameter value. This
    is repeated iteratively until the posterior distribution has been
    sufficiently covered.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Joint Estimation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results}\label{sec:results}
  Lots of plots! $\Delta s$!


%This is the end of analysis section
